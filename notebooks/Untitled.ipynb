{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1877de64-1fed-4185-b241-6f12ce426a1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-02 16:19:13.285523: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-03-02 16:19:13.375747: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:9342] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-03-02 16:19:13.375824: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-03-02 16:19:13.375882: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1518] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-03-02 16:19:13.389673: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-03-02 16:19:13.390362: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-03-02 16:19:20.688507: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "30/30 [==============================] - 21s 408ms/step - loss: 1.1428 - accuracy: 0.7713\n",
      "Epoch 2/50\n",
      "30/30 [==============================] - 13s 421ms/step - loss: 0.6063 - accuracy: 0.7992\n",
      "Epoch 3/50\n",
      "30/30 [==============================] - 11s 373ms/step - loss: 0.5769 - accuracy: 0.7992\n",
      "Epoch 4/50\n",
      "30/30 [==============================] - 12s 412ms/step - loss: 0.5592 - accuracy: 0.8000\n",
      "Epoch 5/50\n",
      "30/30 [==============================] - 12s 412ms/step - loss: 0.5365 - accuracy: 0.8047\n",
      "Epoch 6/50\n",
      "30/30 [==============================] - 12s 418ms/step - loss: 0.4931 - accuracy: 0.8213\n",
      "Epoch 7/50\n",
      "30/30 [==============================] - 11s 367ms/step - loss: 0.4112 - accuracy: 0.8820\n",
      "Epoch 8/50\n",
      "30/30 [==============================] - 12s 412ms/step - loss: 0.3133 - accuracy: 0.9244\n",
      "Epoch 9/50\n",
      "30/30 [==============================] - 12s 417ms/step - loss: 0.2347 - accuracy: 0.9460\n",
      "Epoch 10/50\n",
      "30/30 [==============================] - 13s 432ms/step - loss: 0.1786 - accuracy: 0.9618\n",
      "Epoch 11/50\n",
      "30/30 [==============================] - 12s 413ms/step - loss: 0.1415 - accuracy: 0.9717\n",
      "Epoch 12/50\n",
      "30/30 [==============================] - 11s 369ms/step - loss: 0.1209 - accuracy: 0.9762\n",
      "Epoch 13/50\n",
      "30/30 [==============================] - 13s 426ms/step - loss: 0.1038 - accuracy: 0.9797\n",
      "Epoch 14/50\n",
      "30/30 [==============================] - 13s 428ms/step - loss: 0.0962 - accuracy: 0.9804\n",
      "Epoch 15/50\n",
      "30/30 [==============================] - 13s 428ms/step - loss: 0.0904 - accuracy: 0.9812\n",
      "Epoch 16/50\n",
      "30/30 [==============================] - 11s 380ms/step - loss: 0.0855 - accuracy: 0.9818\n",
      "Epoch 17/50\n",
      "30/30 [==============================] - 13s 421ms/step - loss: 0.0831 - accuracy: 0.9821\n",
      "Epoch 18/50\n",
      "30/30 [==============================] - 14s 453ms/step - loss: 0.0804 - accuracy: 0.9823\n",
      "Epoch 19/50\n",
      "30/30 [==============================] - 13s 429ms/step - loss: 0.0758 - accuracy: 0.9832\n",
      "Epoch 20/50\n",
      "30/30 [==============================] - 14s 454ms/step - loss: 0.0733 - accuracy: 0.9832\n",
      "Epoch 21/50\n",
      "30/30 [==============================] - 12s 409ms/step - loss: 0.0723 - accuracy: 0.9835\n",
      "Epoch 22/50\n",
      "30/30 [==============================] - 13s 406ms/step - loss: 0.0709 - accuracy: 0.9835\n",
      "Epoch 23/50\n",
      "30/30 [==============================] - 13s 430ms/step - loss: 0.0672 - accuracy: 0.9844\n",
      "Epoch 24/50\n",
      "30/30 [==============================] - 13s 424ms/step - loss: 0.0653 - accuracy: 0.9844\n",
      "Epoch 25/50\n",
      "30/30 [==============================] - 13s 433ms/step - loss: 0.0626 - accuracy: 0.9848\n",
      "Epoch 26/50\n",
      "30/30 [==============================] - 14s 466ms/step - loss: 0.0611 - accuracy: 0.9853\n",
      "Epoch 27/50\n",
      "30/30 [==============================] - 13s 405ms/step - loss: 0.0588 - accuracy: 0.9858\n",
      "Epoch 28/50\n",
      "30/30 [==============================] - 13s 440ms/step - loss: 0.0570 - accuracy: 0.9859\n",
      "Epoch 29/50\n",
      "30/30 [==============================] - 13s 449ms/step - loss: 0.0561 - accuracy: 0.9861\n",
      "Epoch 30/50\n",
      "30/30 [==============================] - 13s 434ms/step - loss: 0.0533 - accuracy: 0.9864\n",
      "Epoch 31/50\n",
      "30/30 [==============================] - 13s 424ms/step - loss: 0.0527 - accuracy: 0.9866\n",
      "Epoch 32/50\n",
      "30/30 [==============================] - 12s 384ms/step - loss: 0.0503 - accuracy: 0.9873\n",
      "Epoch 33/50\n",
      "30/30 [==============================] - 13s 448ms/step - loss: 0.0482 - accuracy: 0.9876\n",
      "Epoch 34/50\n",
      "30/30 [==============================] - 13s 435ms/step - loss: 0.0518 - accuracy: 0.9864\n",
      "Epoch 35/50\n",
      "30/30 [==============================] - 13s 439ms/step - loss: 0.0502 - accuracy: 0.9870\n",
      "Epoch 36/50\n",
      "30/30 [==============================] - 13s 434ms/step - loss: 0.0451 - accuracy: 0.9880\n",
      "Epoch 37/50\n",
      "30/30 [==============================] - 12s 392ms/step - loss: 0.0429 - accuracy: 0.9886\n",
      "Epoch 38/50\n",
      "30/30 [==============================] - 13s 443ms/step - loss: 0.0402 - accuracy: 0.9891\n",
      "Epoch 39/50\n",
      "30/30 [==============================] - 13s 435ms/step - loss: 0.0386 - accuracy: 0.9895\n",
      "Epoch 40/50\n",
      "30/30 [==============================] - 13s 431ms/step - loss: 0.0375 - accuracy: 0.9898\n",
      "Epoch 41/50\n",
      "30/30 [==============================] - 14s 462ms/step - loss: 0.0349 - accuracy: 0.9902\n",
      "Epoch 42/50\n",
      "30/30 [==============================] - 13s 435ms/step - loss: 0.0335 - accuracy: 0.9904\n",
      "Epoch 43/50\n",
      "30/30 [==============================] - 13s 411ms/step - loss: 0.0344 - accuracy: 0.9900\n",
      "Epoch 44/50\n",
      "30/30 [==============================] - 14s 456ms/step - loss: 0.0310 - accuracy: 0.9911\n",
      "Epoch 45/50\n",
      "30/30 [==============================] - 13s 440ms/step - loss: 0.0293 - accuracy: 0.9915\n",
      "Epoch 46/50\n",
      "30/30 [==============================] - 13s 437ms/step - loss: 0.0274 - accuracy: 0.9923\n",
      "Epoch 47/50\n",
      "30/30 [==============================] - 13s 434ms/step - loss: 0.0265 - accuracy: 0.9926\n",
      "Epoch 48/50\n",
      "30/30 [==============================] - 12s 397ms/step - loss: 0.0253 - accuracy: 0.9928\n",
      "Epoch 49/50\n",
      "30/30 [==============================] - 14s 458ms/step - loss: 0.0242 - accuracy: 0.9931\n",
      "Epoch 50/50\n",
      "30/30 [==============================] - 13s 423ms/step - loss: 0.0227 - accuracy: 0.9935\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x7f56f2599910>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import re\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "\n",
    "from langdetect import detect \n",
    "from collections import Counter\n",
    "\n",
    "import time\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Bidirectional, Embedding, LSTM, Dense\n",
    "\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "# Chargement du fichier de données\n",
    "file_path = '/home/vatosoa/mg-smart-lingua-discover/data/corpus/sokajinteny.csv'\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Colonnes 'Fehezanteny' et 'Sokajinteny' sont remplies ?\n",
    "df['Fehezanteny'].fillna('', inplace=True)\n",
    "df['Sokajinteny'].fillna('', inplace=True)\n",
    "\n",
    "# Jeton spécial pour les mots inconnus\n",
    "UNKNOWN_TOKEN = '<UNK>'\n",
    "\n",
    "# Fonction de tokenisation de mots avec gestion des mots inconnus\n",
    "def tokenize_words(text):\n",
    "    if isinstance(text, str):\n",
    "        language = detect(text)\n",
    "        text = re.sub(r'[-;:()?!,.\\'\"\\/|]', ' ', text)\n",
    "        tokens = nltk.word_tokenize(text)\n",
    "        tokens = [token.lower() if token.isalnum() else UNKNOWN_TOKEN for token in tokens]\n",
    "        return tokens\n",
    "    else:\n",
    "        return []\n",
    "\n",
    "# Appliquer la tokenisation aux phrases et créer une colonne pour les tokens\n",
    "df['Fehezanteny_tokens'] = df['Fehezanteny'].apply(tokenize_words)\n",
    "df['Sokajinteny_tokens'] = df['Sokajinteny'].apply(lambda x: x.split())\n",
    "\n",
    "# Créer un encodage pour les tokens, y compris le jeton spécial\n",
    "all_tokens = [token for sublist in df['Fehezanteny_tokens'] for token in sublist]\n",
    "token_encoder = LabelEncoder()\n",
    "token_encoder.fit(all_tokens + [UNKNOWN_TOKEN])\n",
    "\n",
    "# Modifier les mots inconnus dans les données d'entraînement et de test\n",
    "X = [[token_encoder.transform([token])[0] if token in token_encoder.classes_ else token_encoder.transform([UNKNOWN_TOKEN])[0] for token in tokens] for tokens in df['Fehezanteny_tokens'].values]\n",
    "y = df['Sokajinteny_tokens'].values\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Création de l'encodeur pour les étiquettes\n",
    "label_encoder = LabelEncoder()\n",
    "label_encoder.fit(np.concatenate(y_train))\n",
    "\n",
    "# Modèle LSTM\n",
    "max_sequence_length = max([len(tokens) for tokens in X_train])\n",
    "vocab_size = len(token_encoder.classes_)\n",
    "num_labels = len(label_encoder.classes_)\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(input_dim=vocab_size, output_dim=64, input_length=max_sequence_length))\n",
    "model.add(Bidirectional(LSTM(128, return_sequences=True)))\n",
    "model.add(Dense(num_labels, activation='softmax'))\n",
    "\n",
    "model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# Entraînement du modèle\n",
    "X_train_padded = pad_sequences(X_train, maxlen=max_sequence_length, padding='post')\n",
    "y_train_padded = pad_sequences([label_encoder.transform(labels) for labels in y_train], maxlen=max_sequence_length, padding='post')\n",
    "\n",
    "model.fit(X_train_padded, y_train_padded, epochs=50, batch_size=32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9eaeab0a-4f40-4dd3-9e82-ddd1e525cd9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8/8 [==============================] - 1s 119ms/step\n",
      "Nombre total de phrases de test: 234\n",
      "Exemple 1\n",
      "Phrase d'origine: Unknown Unknown Unknown Unknown Unknown Unknown Unknown Unknown Unknown Unknown Unknown Unknown Unknown Unknown Unknown Unknown Unknown Unknown Unknown Unknown Unknown Unknown Unknown Unknown Unknown Unknown Unknown Unknown Unknown Unknown Unknown Unknown Unknown Unknown Unknown Unknown Unknown\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "y contains previously unseen labels: ['Mpampitohy']",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 32\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExemple\u001b[39m\u001b[38;5;124m\"\u001b[39m, i\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     31\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPhrase d\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124morigine:\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(decode_tokens(X_test[i], token_encoder)))\n\u001b[0;32m---> 32\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPrédiction:\u001b[39m\u001b[38;5;124m\"\u001b[39m, decode_tokens(decoded_predictions[i], label_encoder))\n\u001b[1;32m     33\u001b[0m \u001b[38;5;28mprint\u001b[39m()\n",
      "Cell \u001b[0;32mIn[4], line 20\u001b[0m, in \u001b[0;36mdecode_tokens\u001b[0;34m(encoded_tokens, encoder)\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecode_tokens\u001b[39m(encoded_tokens, encoder):\n\u001b[0;32m---> 20\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [encoder\u001b[38;5;241m.\u001b[39minverse_transform([token])[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m token \u001b[38;5;129;01min\u001b[39;00m encoder\u001b[38;5;241m.\u001b[39mclasses_ \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mUnknown\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m token \u001b[38;5;129;01min\u001b[39;00m encoded_tokens]\n",
      "Cell \u001b[0;32mIn[4], line 20\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecode_tokens\u001b[39m(encoded_tokens, encoder):\n\u001b[0;32m---> 20\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [encoder\u001b[38;5;241m.\u001b[39minverse_transform([token])[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m token \u001b[38;5;129;01min\u001b[39;00m encoder\u001b[38;5;241m.\u001b[39mclasses_ \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mUnknown\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m token \u001b[38;5;129;01min\u001b[39;00m encoded_tokens]\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/sklearn/preprocessing/_label.py:162\u001b[0m, in \u001b[0;36mLabelEncoder.inverse_transform\u001b[0;34m(self, y)\u001b[0m\n\u001b[1;32m    160\u001b[0m diff \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39msetdiff1d(y, np\u001b[38;5;241m.\u001b[39marange(\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclasses_)))\n\u001b[1;32m    161\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(diff):\n\u001b[0;32m--> 162\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124my contains previously unseen labels: \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m \u001b[38;5;28mstr\u001b[39m(diff))\n\u001b[1;32m    163\u001b[0m y \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39masarray(y)\n\u001b[1;32m    164\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclasses_[y]\n",
      "\u001b[0;31mValueError\u001b[0m: y contains previously unseen labels: ['Mpampitohy']"
     ]
    }
   ],
   "source": [
    "\n",
    "# Prédiction sur les données de test\n",
    "X_test_padded = pad_sequences(X_test, maxlen=max_sequence_length, padding='post')\n",
    "predictions_test = model.predict(X_test_padded)\n",
    "\n",
    "# Décodez les prédictions en étiquettes\n",
    "decoded_predictions = []\n",
    "for i in range(len(predictions_test)):\n",
    "    decoded_sequence = []\n",
    "    for j in range(len(X_test_padded[i])):\n",
    "        if X_test_padded[i][j] != token_encoder.transform([UNKNOWN_TOKEN])[0]:  # Ignorez le jeton spécial\n",
    "            predicted_label = np.argmax(predictions_test[i][j])\n",
    "            decoded_sequence.append(label_encoder.inverse_transform([predicted_label])[0])\n",
    "    decoded_predictions.append(decoded_sequence)\n",
    "\n",
    "# Fonction pour décoder les tokens encodés en chaînes de caractères\n",
    "#def decode_tokens(encoded_tokens, encoder):\n",
    "#    return [encoder.inverse_transform([token])[0] if token != encoder.transform([UNKNOWN_TOKEN])[0] else UNKNOWN_TOKEN for token in encoded_tokens]\n",
    "\n",
    "def decode_tokens(encoded_tokens, encoder):\n",
    "    return [encoder.inverse_transform([token])[0] if token in encoder.classes_ else 'Unknown' for token in encoded_tokens]\n",
    "\n",
    "\n",
    "# Ajouter <UNK> aux classes de l'encodeur d'étiquettes\n",
    "label_encoder.classes_ = np.append(label_encoder.classes_, UNKNOWN_TOKEN)\n",
    "total_test_sentences = len(X_test)\n",
    "\n",
    "# Affichage des exemples de prédictions avec les phrases originales\n",
    "print(\"Nombre total de phrases de test:\", total_test_sentences)\n",
    "for i in range(5):  # Affichage des prédictions pour les 5 premières séquences de test\n",
    "    print(\"Exemple\", i+1)\n",
    "    print(\"Phrase d'origine:\", ' '.join(decode_tokens(X_test[i], token_encoder)))\n",
    "    print(\"Prédiction:\", decode_tokens(decoded_predictions[i], label_encoder))\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c1235f4-178c-4acc-a263-cbca5264cabc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
